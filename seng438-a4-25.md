**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 â€“ Mutation Testing and Web app testing**

| Group\#:       | 25             |
| -------------- | -------------- |
| Student Names: | Justin Pham    |
|                | Joshua Jipp    |
|                | Varshiny Gogul |
|                | Arvind Krishnaa |

# Introduction
This lab report explores two pivotal automated testing methodologies: Mutation Testing and GUI Testing. Mutation Testing assesses test suite effectiveness by injecting faults into the System Under Test (SUT), utilizing Pitest. GUI Testing automates test cases to validate software interfaces, employing Selenium and Sikulix for web interface testing. Through practical experimentation and analysis, we aim to deepen our understanding of these techniques, emphasizing their significance in enhancing software reliability and usability. This report serves as a guide through the process of employing these methodologies, from tool familiarization to test execution and defect documentation.
# Analysis of 10 Mutants of the Range class
<img width="548" alt="image" src="https://github.com/seng438-winter-2024/seng438-a4-justin-phxm/assets/114175397/e40fc5e6-b450-4a52-a21c-5bdee5772f4f">
<img width="551" alt="image" src="https://github.com/seng438-winter-2024/seng438-a4-justin-phxm/assets/114175397/c3aa6b06-960f-42f1-b585-8b6813884cc1">



# Report all the statistics and the mutation score for each test class

[Range class report before modification ](./438MutationTestsBefore.pdf)

![image](https://github.com/seng438-winter-2024/seng438-a4-justin-phxm/assets/113923596/498e99d9-3658-4e51-b262-342621ea533c)


[Range Class report after modification](./438MutationTestsAfter.pdf)

![image](https://github.com/seng438-winter-2024/seng438-a4-justin-phxm/assets/113923596/cfd67480-615a-4efd-bf5d-8a92d2ad5ff9)

For the Range class, we can see that the Mutation coverage increased from 67% to 77%, a 10% increase due to the new Mutants added to the test suite.

[Data Utilities report before modification ](./DataUtilitiesBefore.pdf)
<img width="1189" alt="image" src="https://github.com/seng438-winter-2024/seng438-a4-justin-phxm/assets/114175397/4b3b84e1-1a69-4af7-81e1-0bc8a5f219d2">





# Analysis drawn on the effectiveness of each of the test classes

We systematically tested each functionality with a range of inputs, including valid and invalid data, to ensure robustness and accuracy. This approach allowed us to cover various scenarios and edge cases, enhancing the reliability of our software.

# A discussion on the effect of equivalent mutants on mutation score accuracy

# A discussion of what could have been done to improve the mutation score of the test suites (Josh)
To boost our test suite's mutation score, we concentrated on a few essential strategies, focusing mainly on unit tests. We expanded our test coverage to include more parts of the code, especially targeting areas that were previously neglected, like edge cases and error handling. This approach helped us to evaluate as many scenarios as possible within our unit tests.

We also made sure to strengthen our unit tests, ensuring they were not just passing but also accurately verifying the results. This involved detailed checks to capture subtle errors that mutations might introduce.

A significant part of our strategy was prioritizing the complex and critical sections of our code for testing, as these areas are more susceptible to issues when changes are made. To help identify weaknesses in our unit tests, we used PITest within Eclipse, a mutation testing tool specifically suited for Java projects. PITest was crucial in highlighting the less effective areas of our test suite and where we needed to focus our improvements.

Throughout this process, we consistently reviewed and refined our unit tests, getting the entire team involved in improving our testing approach. By following these steps and leveraging PITest in Eclipse, we significantly enhanced our test suite's mutation score, leading to a more robust and maintainable codebase.


# Why do we need mutation testing? Advantages and disadvantages of mutation testing (Josh)
Mutation testing helps us make sure our tests can catch errors effectively. It works by making small changes to the code, like swapping a plus sign for a minus, to create mutants. Then it checks if our tests can find these changes. The advantage of this is it shows us how good our tests are and where they might be missing errors. It also pushes us to write better tests.

However, mutation testing has some downsides. It can take a lot of time and computer power, especially for big projects, because it has to run tests for many mutated versions of the code. Also, it might create some mutants that don't really affect how the program works, leading to extra work in checking these irrelevant changes.

In summary, mutation testing is useful for improving the quality of our tests, but it can be resource-intensive and sometimes may focus on changes that don't significantly impact the program's behavior.

# Explain your SELENUIM test case design process (Josh)
When designing Selenium test cases, the first step is to identify a specific functionality in the application that needs testing. This involves understanding how this feature is supposed to work and what its critical components are. Once the functionality is clear, the next step is to define scenarios where valid and invalid data is input into the system. This helps in understanding how the application behaves under normal and exceptional conditions.

By comparing these behaviors, we can identify what outcomes should be expected when the system processes correct data versus when it encounters incorrect or unexpected data. During this comparison, it's essential to pinpoint specific Document Object Model (DOM) elements that are unique to each scenario. These elements become key indicators in our Selenium tests to confirm whether the application is responding correctly to the given inputs. By focusing on these unique elements, the test cases can accurately determine whether the functionality is working as intended under various conditions.

# Explain the use of assertions and checkpoints (Josh)
Assertions and checkpoints are used in testing to check if the graphical user interface (GUI) of an application behaves as expected. Assertions are like checkpoints in your test that compare the actual result with the expected result. For example, after clicking a button in the application, you might use an assertion to check if a specific message appears on the screen.

Checkpoints are similar but often refer to specific points in the test where we verify certain conditions, like the presence of a particular image or the correct layout of a webpage. By using these, we can make sure that the GUI looks and works right. If something is off, like a button is missing or a text is wrong, the assertion or checkpoint will fail, alerting us that there's a problem that needs fixing. This way, we increase the odds that users will have a smooth and error-free experience when using the application.

# how did you test each functionality with different test data

# Discuss advantages and disadvantages of Selenium vs. Sikulix

Selenium and Sikulix each serve the same purpose in automation testing, but offer unique benefits and face specific challenges. Selenium works great when it comes to testing web applications across various browsers, such as Chrome and Firefox, and is compatible with frameworks like TestNG, JUnit, and NUnit. This simplifies test management and allows for a high degree of customization at no cost. Selenium is mainly geared towards web environments, leaving desktop applications a bit out of its reach. Furthermore, its reliance on programming know-how might sideline testers who aren't as code-savvy, and occasionally, its tests may stumble without clear ties to the actual application issues.

Sikulix differs from Selenium in that it has image recognition, enabling testers to interact with user interfaces without needing the source code. This makes Sikulix a preffered tool for desktop automation across different operating systems like Windows, Mac, and Linux, offering a gentler learning curve for those less familiar with coding. However, Sikulix's focus on visual elements does mean it's not the best fit for web automation tasks. Its performance can lag with larger or more complex setups, and significant UI changes might throw it off course.

# How the team work/effort was divided and managed

Justin and Joshua worked on increasing mutation coverage for the Range class and GUI tests for ebay adding to cart, buy it now, search functionality.

Arvii and Varshiny worked on increasing mutation coverage for the Data Utilities class and GUI tests for ebay login, feedback, and signup section.

Each group worked on analysis of 5 mutants for Range class for a total of 10 mutant analyses.

# Difficulties encountered, challenges overcome, and lessons learned

A huge hurdle that we had in this lab was from the mutation testing. The hurdle came from running the mutation tests where we waited very long for the mutation tests to run. Some group members had to wait up to 15 minutes for the mutation tests to finish and some group members only had to wait around 2 minutes. We thought this may have had to do with the operating system environment since the MacOS users had to wait longer and Windows users wait time was short. In addition, we also found the IDE setup very difficult. The instructions on how to setup and run PITEST were very unclear and we spent hours of trying to figure out how to run the mutation tests. In the end, we solved the issue by placing all the necessary files into a single package and the mutations ran fine.

# Comments/feedback on the lab itself

The lab was an excellent opportunity to explore Mutation testing and GUI testing. The lab could have been further improved if the instructions to setup the mutation tests were more clear.
